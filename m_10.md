# M10: Reporting & Tracking

## Purpose
Package results into a reproducible research report and establish lightweight experiment tracking. Capture configs, code state, and outputs; render tables/plots (J–K heatmaps, equity, drawdowns, alpha tests) and persist run manifests for full auditability.

## Objectives
- Generate a research report with tables and figures covering performance, alpha, subperiods, and robustness (per momentum_TRD.md §10).
- Track experiments: log config snapshot/hash, commit hash, seeds, inputs/outputs, and key metrics to CSV/JSON.
- Provide deterministic, re-runnable pipelines and clear artifact locations for audit and handoff.
- Add config schema for `report.*` and `tracking.*` with sensible defaults and validation.

## Scope
- In-scope: report assembly (tables/plots), artifact manifests, run logging, config model updates, optional CLI, tests, and docs.
- Out-of-scope: web dashboards, MLflow integration, interactive notebooks, and advanced templating engines (can be future work).

---

## Inputs and Outputs

### Inputs
- Clean data and results from prior milestones:
  - `data/clean/ohlcv.parquet` (M1)
  - `data/clean/momentum.parquet` (M4)
  - `data/clean/portfolio_holdings.parquet`, `portfolio_trades_costed.parquet` (M5–M6)
  - `data/clean/backtest_daily.parquet`, `backtest_monthly.parquet` (+ optional `holdings_daily.parquet`, `executions.parquet`) (M7)
  - Metrics outputs (M8): computed in code via `src/metrics.py`/`src/stats.py`
  - Robustness/CV outputs (M9): `data/clean/cv_results.csv`, `cv_selection.csv`, `cv_oos_summary.csv`, `robustness_costs.csv`, `robustness_subperiods.csv`
- Config: extend `config/data.yml` with sections (validated):
  - `report`: output dirs, figure sizes, which plots/tables to include, and file formats
  - `tracking`: run log path, save snapshots/hashes, and artifact policy

### Outputs
- Report bundle under `data/clean/report/`:
  - `momentum_report.md` (primary, deterministic Markdown)
  - `figs/` images: `equity_curve.png`, `drawdown.png`, `turnover.png`, `jk_heatmap.png`, `subperiod_stats.png`, `alpha_regression.png` (optional: `capacity_cost_curve.png`)
  - `tables/` CSVs: `monthly_returns.csv`, `subperiod_metrics.csv`, `alpha_summary.csv`, `cv_oos_summary.csv` (copied or linked from M9), `robustness_costs.csv` (copied)
- Experiment tracking under `data/clean/experiments/`:
  - `runs.csv`: one row per run with config/commit hashes, timestamps, seeds, and headline metrics
  - `runs/<run_id>/manifest.json`: paths and SHA256 hashes of key inputs/outputs
  - `runs/<run_id>/config_snapshot.yml`: exact config used
  - `runs/<run_id>/artifacts/` (optional): copies of small derived CSV summaries used by the report

Notes
- Prefer symlinks or path references to large artifacts instead of copying to avoid duplication. Hash files for integrity.

---

## Report Contents
1. Strategy Overview
   - One-paragraph description, assumptions, data lineage diagram (text or simple ASCII), and timing conventions (skip, formation, holding).
2. Performance Summary (OOS focus where applicable)
   - CAGR, vol, Sharpe, IR vs VN-Index, max drawdown, hit-rate; monthly returns table with totals.
3. Equity & Drawdowns
   - Equity curve versus VN-Index; drawdown plot and summary stats.
4. Trading & Capacity
   - Turnover over time; optional capacity proxy (e.g., cost vs participation or ADV cap scenarios if configured in robustness).
5. Alpha & Inference
   - OLS alpha vs market; Newey–West SE (lags=6 monthly); t-stats and confidence intervals; brief interpretation.
6. Robustness & CV (from M9)
   - J–K heatmap (Sharpe or chosen metric), selection frequency, OOS dispersion; subperiod/regime metrics; cost sensitivity curves.
7. Reproducibility & Audit
   - Commit hash, config snapshot/hash, environment info, inputs/outputs manifest with file hashes; notes on determinism.
8. Limitations & Next Steps
   - Data/market caveats (halts, limits approximations), sensitivity to costs and turnover; outline future extensions.

---

## Algorithm/Assembly Details
1. Load Inputs
   - Read backtest and metrics data; lazily load CV/robustness CSVs if present in config; short-circuit gracefully if optional items missing.
2. Compute/Collect Tables
   - Monthly returns and totals from `backtest_monthly.parquet`.
   - Subperiod slices using configured ranges; compute per-slice metrics via `src/metrics.py`.
   - Alpha regression summary via `src/stats.py` with NW SE.
   - J–K grid tables from `cv_oos_summary.csv` (or recompute summary from `cv_results.csv`).
3. Generate Plots (matplotlib/seaborn)
   - Equity and drawdown from daily series; turnover from monthly or executions; heatmap from CV summary; cost curves from robustness CSV.
4. Compose Markdown
   - Render a deterministic Markdown report with sections above, embedding image paths and linking CSV tables; avoid RNG or current-time beyond run metadata.
5. Persist Artifacts
   - Write figures/tables under `data/clean/report/` and record all artifact paths in manifest.
6. Track Run
   - Compute SHA256 of config and inputs; capture `git` commit hash (if available); log to `runs.csv`; write `manifest.json` and `config_snapshot.yml` under `runs/<run_id>/`.

---

## Configuration
Add to `config/data.yml` (validated via Pydantic, with defaults):

```yaml
report:
  out_dir: "data/clean/report"
  figs_dir: "data/clean/report/figs"
  tables_dir: "data/clean/report/tables"
  include:
    equity_curve: true
    drawdown: true
    turnover: true
    jk_heatmap: true
    subperiods: true
    alpha_regression: true
    capacity_curve: false
  figure_dpi: 150
  figsize: [9, 5]

tracking:
  runs_dir: "data/clean/experiments"
  runs_csv: "data/clean/experiments/runs.csv"
  hash_inputs: true
  hash_outputs: true
  capture_commit: true
  capture_env: true  # python version, packages snapshot (freeze)
```

Validation
- Paths must be strings; `figure_dpi >= 72`; `figsize` two positive numbers; booleans for `include.*`.
- `runs_dir` and `runs_csv` must be under `data/clean/` to keep artifacts versionable but out of source.

Backward compatibility: If `report`/`tracking` omitted, default to minimal report in `data/clean/report/` and append-only `runs.csv`.

---

## CLI (Optional)
Add:
- `scripts/make_report.py`
  - Args: `-c/--config` (default `config/data.yml`), `--run-id` (auto timestamp if omitted), `--dry-run`.
  - Behavior: assembles report bundle per config; writes Markdown + figs/tables and updates tracking.
- `scripts/log_experiment.py`
  - Args: `-c/--config`, `--run-id`, key metrics via flags (or read from latest outputs).
  - Behavior: logs/updates `runs.csv` and writes manifest only (no plots).

Usage examples
- `poetry run python scripts/make_report.py -c config/data.yml`
- `poetry run python scripts/make_report.py -c config/data.yml --run-id 2025-08-01_OOS`

---

## Tests
Create `tests/test_reporting.py` and `tests/test_tracking.py`.

- Unit: table assembly
  - Monthly returns table sums to reported totals; subperiod filters match date ranges; alpha summary has required fields and NW t-stats.
- Unit: plotting hooks
  - Plot functions emit files when `include.*=true`; filenames match config; deterministic contents (smoke tests on size/hash length).
- Unit: config validation
  - Pydantic model accepts defaults; rejects invalid `figsize` and non-under-`data/clean` paths.
- Unit: tracking manifest
  - `manifest.json` lists inputs/outputs with SHA256; `runs.csv` appends a new row; commit/config hashes non-empty.
- Integration: end-to-end
  - On toy data, `make_report.py` creates `momentum_report.md`, `figs/`, `tables/`, and logs a run; Markdown contains expected section headers and image links.

Testing helpers
- Use `tmp_path`; mock `git` if repository metadata unavailable; seed via `set_seed` from `src/utils.py`.

---

## Acceptance Criteria
- Report bundle produced with all configured plots and tables; Markdown renders without broken links.
- `runs.csv` updated with a new entry capturing timestamp, run_id, commit hash (if present), config hash, seeds, and headline metrics.
- `manifest.json` includes paths and SHA256 hashes for key inputs and outputs; `config_snapshot.yml` stored.
- Deterministic: identical inputs/config produce identical report and tracking artifacts (excluding timestamps/run_id if configured).
- `poetry run pytest -k "report or tracking" -q` passes for new tests.

---

## Integration with Previous Milestones
- M1: Reuse `src/reports.py` patterns for writing CSVs; include coverage/anomalies references in report appendix if desired.
- M4–M7: Consume signals, holdings, and backtest outputs for tables/plots.
- M8: Use metrics and alpha statistics APIs; embed summaries.
- M9: Pull CV/robustness CSVs for heatmaps and sensitivity plots.

---

## Non-Functional Requirements
- Determinism: no RNG; stable sorting and joins; fixed figure DPI and fonts.
- Performance: plotting and table assembly on multi-year data completes within seconds to a couple of minutes on a laptop.
- Portability: no network or external services; pure Python/pandas/matplotlib.
- Auditability: all artifacts discoverable via `runs.csv` and per-run manifests.

---

## Implementation Notes
- Keep report assembly in `src/reports.py` to centralize outputs (extend existing helpers) and add `src/experiment_tracking.py` for manifests and run logging.
- Compute hashes with streaming reads to avoid memory blowups; skip hashing very large files if disabled.
- When `git` not available, mark `commit` as `unknown` and continue.
- Prefer relative paths in manifests (rooted at repo) for portability.
- Keep images simple and consistent; annotate plots minimally to remain legible in Markdown.

---

## Deliverables
- Code: extensions to `src/reports.py`; new `src/experiment_tracking.py`; config model updates.
- CLI: `scripts/make_report.py` (and optionally `scripts/log_experiment.py`).
- Tests: `tests/test_reporting.py`, `tests/test_tracking.py` with unit and e2e coverage.
- Data: `data/clean/report/` (Markdown, figs, tables) and `data/clean/experiments/` (runs.csv, per-run manifests/snapshots).
- Docs: This `m_10.md` and function docstrings.

